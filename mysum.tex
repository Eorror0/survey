\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{tmi}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\journalname, VOL. XX, NO. XX, XXXX 2020}
{Author \MakeLowercase{\textit{et al.}}: Preparation of Papers for IEEE TRANSACTIONS ON MEDICAL IMAGING}
\begin{document}
\title{Preparation of Papers for IEEE TRANSACTIONS ON MEDICAL IMAGING}
\author{First A. Author, \IEEEmembership{Fellow, IEEE}, Second B. Author,and Third C. Author, Jr., \IEEEmembership{Member, IEEE}
\thanks{This paragraph of the first footnote will contain the date on which
you submitted your paper for review. It will also contain support information,
including sponsor and financial support acknowledgment. For example, 
``This work was supported in part by the U.S. Department of Commerce under Grant BS123456.'' }
\thanks{The next few paragraphs should contain the authors' current affiliations,
including current address and e-mail. For example, F. A. Author is with the
National Institute of Standards and Technology, Boulder, CO 80305 USA (e-mail:author@boulder.nist.gov). }
\thanks{S. B. Author, Jr., was with Rice University, Houston, TX 77005 USA.
He is now with the Department of Physics, Colorado State University,
Fort Collins, CO 80523 USA (e-mail: author@lamar.colostate.edu).}
\thanks{T. C. Author is with the Electrical Engineering Department,
University of Colorado, Boulder, CO 80309 USA, on leave from the National
Research Institute for Metals, Tsukuba, Japan (e-mail: author@nrim.go.jp).}}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
Enter about five key words or phrases in alphabetical order, separated by commas.
Cancer survival analysis,Cancer survival prediction,deep learning,whole slide image
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
% motivation 添加 
Survival prediction is a direction of statistics for analyzing the duration time that is expected until the events of interest occur,such as the death of the life form of biology.
% 癌症是当前人类面临的最大的健康危机之一，包括各种类型的癌症，乳腺癌，肠胃癌，肺癌等等。临床领域为了减少癌症的危害，可以通过使用wsi类型的切片，由anatomic pathologists使用haematoxylin and eosin (H&E)方法染色的一种组织切片，进行生存预测。生存预测的使用能使得临床医生能够更好地了解患者病情，给出更好的治疗方案，因此能够提高患者的存活率和存活年限。
With the rising importance of precision medicine, the ability to predict patient survival based on individualized characteristics becomes paramount.  WSI, as a rich source of information, offers an unprecedented level of granularity by capturing the heterogeneity within tissue samples. By enabling the high-resolution digitization of entire tissue slides, WSI has not only enhanced the efficiency and accuracy of pathological assessments but has also opened new avenues for predictive modeling and survival analysis.

This review aims to provide a comprehensive overview of the application of WSI in the context of survival prediction. We delve into the intersection of digital pathology and computational methods, exploring how WSI data can be harnessed to predict patient outcomes, especially in the field of oncology.

The introduction of WSI has given researchers and clinicians access to vast repositories of image data, offering new opportunities for feature extraction, machine learning, and deep learning techniques to model and predict patient survival. The ability to analyze tissue morphology, cellular structures, and patterns within whole slide images has the potential to uncover valuable insights into disease progression and prognosis.

In this review, we will examine the key methodologies, challenges, and recent advancements in survival prediction using WSI data. We will also discuss the implications of such predictions on personalized medicine, treatment strategies, and patient care.Additionally, we will explore various aspects of survival prediction using WSI, including the development of novel image features, the utilization of deep learning architectures for image analysis, and the challenges associated with large-scale data management.  We will also consider the ethical and regulatory considerations in implementing such predictive models in clinical practice.

As the fields of digital pathology and computational biology continue to evolve, the integration of WSI into survival analysis represents a promising frontier for improving patient outcomes and advancing our understanding of disease dynamics. This review will serve as a guide for researchers and practitioners interested in leveraging WSI for survival prediction in diverse clinical and research settings.

\section{Background}
% 癌症风险预测是指对潜在的病人的全切片WSI进行分析，也可以结合病人的其他临床数据，例如年龄、性别等，得出病人直至死亡的存活时间。
% 在生存预测问题中，所使用的数据集中，所使用的censorship和survival time基本都是右删失。
In recent years, several computer algorithms for hematoxylin and eosin (H\&E) stained pathology image analysis have been developed to aid pathologists in objective clinical diagnosis and prognosis.
The Cancer Genome Atlas (TCGA) are the most common datasets when discussing the topics of survival prediction.

the observation of one patient is either a survival time $(O_i)$ or a censored time $(C_i).$ If and only if $t_i=\min(O_i,C_i)$ can be observed during the study, the dataset is right- censored [17]. An instance in the survival data is usually represented as $(x_i,t_i,\delta_i)$ where $x_i$ is the feature vector, $t_i$ is the observed time, $\delta_i$ is the indicator which is 1 for a un- censored instance (death occurs during the study) and 0 for a censored instance.

The survival function $S(t|x)=Pr(O\geq t|x)$ is used to identify the probability of being still alive at time $t$ where $x=(x_1,...x_p)^T$ is the covariates of dimension $p$, The haz- ard function is defined as

In recent years, the integration of Whole Slide Imaging (WSI) into the realm of survival prediction has emerged as a promising frontier in the fields of digital pathology, oncology, and computational biology. WSI, a revolutionary technology, has transformed the way we analyze tissue samples, providing a comprehensive digital representation of entire pathology slides.

Traditionally, pathological assessments have been reliant on manual inspection and subjective interpretation of glass slides under a microscope. While this approach has served as the gold standard for disease diagnosis and prognosis, it is inherently limited by issues of inter-observer variability, labor intensiveness, and the inability to harness the full potential of the vast data embedded in tissue structures.

The advent of WSI has paved the way for a paradigm shift in tissue analysis. Through high-resolution digitization, WSI generates massive datasets of tissue samples, allowing for a more detailed examination of cellular structures, tissue morphology, and spatial relationships. These digital representations of tissue hold a wealth of information that extends far beyond what can be appreciated by the human eye.

One of the most compelling applications of WSI lies in the realm of survival prediction, particularly in the context of cancer. Cancer remains a leading cause of mortality worldwide, and the ability to predict patient outcomes accurately is paramount for optimizing treatment strategies and improving patient care. With the integration of WSI, researchers and clinicians can extract rich image features and employ advanced machine learning and deep learning techniques to model and predict patient survival.

Survival prediction using WSI offers the potential to uncover subtle patterns, biomarkers, and prognostic factors that may have gone unnoticed in traditional pathology assessments. The granularity and comprehensiveness of WSI data open doors to new avenues of research and personalized medicine, enabling clinicians to tailor treatments to individual patient profiles.

In this review, we delve into the intricate interplay between digital pathology and computational methods, exploring how WSI is being harnessed to predict patient survival. We will examine the methodologies, challenges, ethical considerations, and the implications of these predictions on clinical practice.

As the field of WSI continues to evolve, the fusion of digital pathology and survival prediction holds great promise for advancing our understanding of disease dynamics and ultimately enhancing patient outcomes.

As we delve deeper into this evolving landscape, it is crucial to appreciate the monumental shift brought about by WSI. Its capacity to create digital archives of pathology slides has not only expedited the diagnostic process but has also catapulted computational pathology to the forefront. This dynamic shift, driven by advancements in digital imaging and artificial intelligence, has redefined the scope of pathology by offering a more profound understanding of disease, bolstering diagnostic accuracy, and heralding a new era of prognostic modeling.

WSI's role in survival prediction is particularly significant within the domain of cancer research. It has given rise to the emergence of predictive models that harness the extensive image data contained within tissue samples. By analyzing and extracting pertinent information from these digitized slides, researchers can identify morphological and structural biomarkers, which, when combined with clinical data, offer valuable insights into patient outcomes.

Moreover, the integration of WSI data provides the foundation for the development of prognostic models capable of guiding treatment decisions and improving the overall quality of patient care. The implications extend beyond individualized medicine to population-level studies, ultimately influencing healthcare policy, resource allocation, and public health strategies.

While the potential is immense, it is not without challenges. The management and analysis of vast WSI datasets demand advanced computational infrastructure, robust machine learning algorithms, and a harmonious partnership between pathologists and data scientists. Ethical considerations surrounding patient data privacy, model interpretability, and regulatory compliance also merit attention.

This review embarks on a journey to explore the multifaceted landscape of survival prediction using WSI. It endeavors to survey the methodologies, showcase the milestones achieved, and address the formidable challenges that lie ahead. In doing so, it aspires to serve as a guiding compass for researchers, pathologists, clinicians, and healthcare stakeholders invested in leveraging the symbiotic relationship between digital pathology and survival prediction.

As we advance in this ever-evolving field, the fusion of digital pathology with survival prediction promises to unravel the intricacies of disease dynamics, enhance patient care, and pave the way for a new era of data-driven medicine.

\subsection{Abbreviations and Acronyms}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have already been defined in the abstract. Abbreviations 
such as IEEE, SI, ac, and dc do not have to be defined. Abbreviations that 
incorporate periods should not have spaces: write ``C.N.R.S.,'' not ``C. N. 
R. S.'' Do not use abbreviations in the title unless they are unavoidable 
(for example, ``IEEE'' in the title of this article).

\section{Methodology}
\subsection{Region of Interest}
There exist many methods developed for predicting survival through the information provided by the whole slide images(WSIs).
Rather than utilizing the overall patches from the gigapixel pathology images, the traditional models usually pre-select a subset of critical patches from the region of interest (ROI) as the input data.
Apart from the features extracted from ROIs with the deep neural networks(DNNs), some morphological features of the image patches can be extracted and accessed by the image analysis software named CellProfiler\cite{lamprecht2007cellprofiler}, commonly used for cell phenotypes measurements at that time.
It's the features including cell shape, size, the distribution of pixel intensity in the cells and nuclei and texture of cells and nuclei that the quantitative analysis tool can extract.
% DeepConvSurv
% To evaluate whether
Taking whole slide images with various size as inputs, in order to do end-to-end survival prediction, DeepConvSurv proposed by \cite{zhu2016deep} randomly chooses the patches among the ROIs annotated by the professional pathologists.
And the experiments showed it could extract more abstract information different from the hand-crafted features generated by the state-of-the-art analysis tool CellProfiler mentioned above.
% convnet [6], [8], [9], [10], [11]
Except this, \cite{zhu2017wsisa}, \cite{di2020ranking}, \cite{yao2020whole}, \cite{abbet2020divide}, \cite{yao2019deep} have shown that random sampling of patches within the tissues in WSIs still makes sense in stratifying phenotypic information which can be improved.
% Predicting cancer outcomes
% in this paper；results；
With the help of online tool, Mobadersany et al.\cite{mobadersany2018predicting} manually selected the ROIs without tissue-processing artifacts containing overstaining or understaining areas from alternates, which have viable tumor features. 
Faced the difficulty of intratumoral heterogeneity and few availability of labeled data, to obtain better and more robust effects, the model uses the data augmentation techniques and sorted median risks to get prediction results.
And the model eventually gets more accurate outcomes. 
Moreover, the paper provides some publicly accessible datasets with ROIs.
% Pathomic Fusion；microenvironment 
With the assistance of the assembled diagnostic slides offered by \cite{mobadersany2018predicting} with ROIs, characteristics of tumor microenvironment could be got in the built graphs in \cite{chen2020pathomic}.
Because semantic segmentation is executed in ROIs to recognize and localize relevant cells acting as set of nodes in the spatial graph for abstract graph representations.
% deepcorrsurv
Not only dose the model in \cite{yao2017deep} use the public cancer survival dataset TCGA, but it also adopts a core sample set from UT MD Anderson Cancer Center during the holistic procedure.
And it takes advantage of the annotations of ROIs to locate the possible tumor regions in pathological images for subsequent steps.
% Comprehensive analysis
% In this study；In the NLST dataset；
% esat中提到分类
Some methods adopt sampling strategy to generate candidate patches not limited to ROIs.
Since revealing ROIs requires specialized prior knowledge and expensive labor costs, Wang et al. proposed an automatic model aimed at finding ROIs.
The proposed model in \cite{wang2018comprehensive} can identify tumor regions as ROIs in hematoxylin and eosin (H\&E) stained pathology images using predicted likelihood of image patches, each patch tagged as the highest probability category. 
In this way, some tumor area-related features can be extracted as the descriptors of above ROIs including area, perimeter, convex area, filled area, major axis length, minor axis length and so on.
For the purpose of training the prognostic model indicating that the risk group defined by tumor shape features is an independent prognostic factor, the features are used in the training process.
% capsurv
The model in \cite{mackenzie2022neural} has a automated pipeline excluding the background white space among the identified tissue, then overlooking the sparse cellularity regions and randomly sampling the potential patches from the foreground area.

In a short, as the methods previously implemented, ROIs routinely ask artificial marking and rigorous reviews to produce passable survival prediction.
Additionally speaking, the ROI-based methods discussed above require pathologists to hand-annotate ROIs, a tedious task.
\subsection{Feature Extraction}
Previous methods often extract image features from patches of whole slide images(WSIs) using the pre-trained model based on the numerous natural images datasets ImageNet theoretically being able to withdraw the low-dimensional features such as edge and texture.However,they have ignored the enormous difference between the WSIs and the natural images.
Recently, some new methods have been proposed to suitably get features from the WSIs to overcome the significant shortcomings.
Without the knowledge of each patch-level labels, the self-supervised learning methods can autonomously impart the outstanding feature extraction ability to the model.

% Self supervised contrastive
% abstract
The method named SimCLR in \cite{ciga2022self}, one of the self-supervised learning(SSL) methods, using the contrastive learning has excellent feature extraction capacity even comparable to the supervised learning model thanks to the collected 57 digital histopathology datasets with none labels.
% Integration of Patch；Train Feature Extraction Model；Recent studies have shown；
SeTranSurv, the proposed model in \cite{huang2021integration} applies SimCLR to train the initial feature extraction model ResNet18\cite{he2016deep} to get the specialized model for our downstream task survival prediction.
During the training the model applies the contrastive loss\cite{he2020momentum} to enhance feature extraction ability.
Firstly augmentation module in SeTranSurv using two methods to transform a random image example to a position pair, then the model uses the fine-tuned feature extractor raised above to get the features. 
Considering other images as negative examples ensures that the views of different slide images are far apart in the high-dimensional space and the views of the same image are closer in the process of training.
Additionally, position encodings capturing spatial information and self-attention modules learning correlation between patches are put into the training of the model above to obtain slide-level features and therefore the patient-level features.
% Fine-Tuning and training
% higt，hgcn直接用；hyper adac，ds-mil用simclr作为策略
Aside from SimCLR as a strategy used in \cite{benkirane2022hyper} and \cite{li2021dual}, KimiaNet from \cite{riasatian2021fine} has also been one of the most welcome pre-trained models used for survival analysis exploiting a variable, multi-organ open image repository lick TCGA, which has been employed directly to extract the feature embeddings of image patches in \cite{guo2023higt} \cite{li2021dual}.
% Cancer Survival Prediction即convnet
% Feature Extraction Via
Without using convnet-based methods, the model in \cite{fan2022cancer} also considers the self-supervised learning (SSL) methods mainly for making full use of plentiful color information designed for WSI patches or pixels without hand-actuated labels, totally colorization and cross-channel as the pretext tasks.
The colorization model is trained to predict corresponding color channels based on the lightness channel and the letter, on the other hand, is trained to get lightness channel using given color channels data, after which the visualized results indicating highlighted overall structure of nuclei and tissue.

\subsection{Multimodal}
One limitation for some existing survival models is that they initially focus on one modality and cannot sufficiently handle multi-modalities data. 
Actually, multi-modalities information could provide complementary and auxiliary information for tumor diagnosis.
For instance, molecular data and whole slide images share relevant representations to describe the same event in tumor growth and symptoms which are very critical for tumor diagnosis.
Therefore,it is essential and necessary to combine and integrate multi-modal data such as pathological images, genotypic information and clinical data for explaining and understanding cancerous heterogeneity and complex symptoms for customized treatments and healings, consequently boosting the survival predictions.
Although hematoxylin and eosin (H\&E)-stained slides are enough to build a comprehensive diagnosis, other modal data can provide a deeper description of the tumor. For example, genomic profiles being comprising of ten thousand dimensional sequences can provide a molecular characterization of the tumor.
Additionally,during the past several decades, multiple clinicians have made clinical cancer survival prediction on the basis of clinical variates and experience, therefore the clinical data is also an important source modality for multi-modal survival prediction.
However, multi-modal survival prediction faces an important challenge due to the huge data heterogeneity gap between WSIs and other modalities, and many proposed approaches use simple multi-modal fusion mechanisms for feature incorporation, which give up mining important multi-modal relationships.

% LUNG CANCER SURVIVAL PREDICTION
The model in \cite{zhu2016lung} gets the features from the pathological images extracted using Cellprofiler\cite{lamprecht2007cellprofiler} from the image tiles in ROIs,namely geometry, texture, holistic features.In addition,the model uses the preprocessed genetic data and one feature selection operation SPACE\cite{peng2009partial} to select representative features to integrate data for the principal component regression model for survival prediction.
The experiments focusing on ADC lung cancer revealed that the results could be better than only using data of genes or images, which demonstrated that the genetic data could actually enhance the prediction performance of the survival analysis model to some extent.

% DeepCorrSurv
Differently, the experiments in \cite{yao2017deep} were conducted on other two cancer types: glioblastoma multiforme (GBM) and lung squamous cell carcinoma (LUSC) using pathological images and molecular data including protein data, Copy number variation(CNV) data and so on. 
But similarly, to fuse the data from different modality for better results, the model firstly learn deep representations from two kinds of data using separate Convolutional Neural Networks (CNNs).Next, the representations passing through the sub-network in the model are connected to get the new representation, which serves as the input of the correlational layer.
Certainly, the deep correlation layer is used to decrease the discrepancy by maximizing the correlation.
After the common layer, the output acts as the input of the survival prediction layer using the negative log partial likelihood as survival loss function.
Compared with the models handling the linear condition, the new model DeepCorrSurv can learn complex correlation using deep neural networks by using the unsupervised method to learn the interactions and the survival loss network to fine-tune the model, eventually getting better results in the comparison experiments about lung and brain cancer.
The results showed that the common representation after maximizing step could bring better performance to the survival prediction measured by the metric named concordance index values or c-index values.

% pnas
In the paper\cite{mobadersany2018predicting}, the model named GSCNN aims for fusing genomic data and second modal data from The Cancer Genome Atlas (TCGA) Lower-Grade Glioma (LGG) and Glioblastoma (GBM) projects. 
If the second modal data are added to the network during the whole training process, the median c index will improve more than simply integrating the second type data into the fully connected layers. Then the model proves that the Molecular subtype is significant in the multi-variable regression model.

% GPMKL
Again, for genomic information, Sun et al. in \cite{sun2018integrating} considered building distinct models to make survival predictions according to the five major molecular subtypes of the breast cancer.
The genomic data chosen in the model consists of gene expression, copy number alteration (CNA), gene methylation and protein expression.
The main idea in the paper is how to merge different types of data as one type, and in this way using multiple kernel learning (MKL) is a choice.
To integrate the genotypic information and image data, GPMKL model was proposed using 5 independent Gaussian kernels and having a integrating step.
The baseline algorithms used that time for comparison includes LASSSO-Cox\cite{tibshirani1997lasso}, elastic-net penalized Cox (EN-Cox)\cite{yang2013cocktail}, Parametric censored regression models (PCRM)\cite{kalbfleisch2011statistical}, Random survival forests (RSF)\cite{ishwaran2008random}, Boosting concordance index (BoostCI)\cite{mayr2014boosting}, Supervised principal components regression (superPC)\cite{bair2006prediction}.
Additionally, two independent models named GMKL and PMKL was constructed only adopting genomic data or pathological images data and four single dimensional models using four types of genomic data were also built. 
Subsequently, the results showed that the gene expression and protein information play relatively more important role than others and CNA makes a little contribution to holistic prediction accuracy.

% MultimodalPrognosis
In the model proposed in \cite{cheerla2019deep} named MultimodalPrognosis, clinical, genomic and WSI images data are processed by FC layers, deep highway networks\cite{srivastava2015highway}, the SqueezeNet architecture\cite{iandola2016squeezenet} separately.
The microRNA data is also passed through deep highway networks discussed. 
One notable problem about the microRNA and clinical data is the missing data.
However, the function of the highway networks dose not stand out without comparison with other methods in the paper.
Then, the similarity loss is used to train the model to recognize the patient-distinguishing patterns and correspond the data from different modality to generate associated representations. 
Thus, the final loss function is composed of cox loss and similarity loss in the unsupervised model.
The multi-modal dropout was also invented that is dropping whole feature vectors of each modality and accordingly increase the weights of other modalities to build robust representations.
The solution above was then validated in the experiments including visualizing the encodings of the pancancer patient cohort and calculating the C-index values and the results also demonstrated the essence of the adopted modalities.

% OSCCA
One more variation usually ignored is the ordinal relationship among the survival time of different patients, which is assumed independent among patients.
The model proposed in \cite{shao2018ordinal} named OSCCA intends to take advantage of the information.
For extracting features, the model uses the methods in \cite{phoulady2016nucleus} to generate segmented nucleus and get the specific features.
As to gene expression data, the co-expression network analysis algorithms are utilized to derive eigen-gene features.
Considering the correlation between imaging and eigen-gene features, sparse canonical correlation analysis(i.e. SCCA) is used to choose features.
Among the datasets, most patients are censored, denoting that their actual living time is longer than recorded data, while the uncensored patients have the real survival time.
To make full use of the censored state, an ordinal sparse canonical correlation analysis (OSCCA) method is proposed.
In the newly proposed method, the equation estimates the uncensored information, while linear inequalities restrains the ordered relationship between censored and uncensored data. 

% OMMFS
After the model above, Shao et al. developed a new model named OMMFS in \cite{shao2019integrative} using CNV data and level-3 DNA Methylation (DME) data additionally.
The experiments showed that the log-rank test\cite{cheng2017integrative} had better stratification performance than univariate Cox regression method.
Furthermore, the model implements the second feature selection function based on the Generalized Sparse Canonical Correlation Analysis (GSCCA) framework\cite{witten2009extensions} to get the inherent relationship among different modalities.
Likewise the modality in OSCCA, the model under discussing also notices the survival information of patients.
The validation experiments demonstrated that the new model even had superior stratification of early-stage KIRC patients.
To make comparison, the SGSCCA model was proposed with the same objective function without ordered survival information.
However, the feature pre-selection strategy is based on the median value, which is too arbitrary to consider the accurate relevance between features and cancer type.
Also, the image features relies on the regions of interest annotated by pathologists.

% PAGE-Net；gpt
This model in \cite{hao2019page} has a two-stage feature extraction process used in a deep learning model for pathology-specific layers. 
In the first stage, a pre-trained convolution neural network (CNN) is used to identify survival-discriminative features in patches of pathological images. 
These features are obtained through dilated convolution layers and max-pooling. 
In the second stage, global survival-discriminative features for a whole slide image (WSI) are generated by aggregating feature scores from multiple patches. 
A two-stage pooling approach, including 3-norm pooling, is used to rank and aggregate the most important features and patches. 
The resulting vector of aggregated survival-discriminative features represents a WSI for a patient, contributing to the integrative deep learning model.
The genome and demography-specific layers in this model are adapted from the Cox-PASNet\cite{hao2018cox}, which is a pathway-based sparse deep neural network. 
The genome-specific layers consist of a gene layer, a pathway layer, and two hidden layers (H1 and H2). 
The gene layer serves as an input layer for gene expression data, with each node representing a gene. 
The pathway layer incorporates prior biological knowledge from databases like KEGG for biological interpretation. 
Connections between the gene layer and pathway layer are established based on biological pathway databases, with pathway nodes representing specific biological pathways. 
The two hidden layers capture nonlinear and hierarchical relationships between the pathways.
Clinical patient data are integrated into the demography-specific layer and combined with genomic features from gene expressions and aggregated survival-discriminative features from pathological images in the final hidden layer of the integrative model. 
To address overfitting in deep learning models with high-dimensional, low-sample-size data, the training technique from Cox-PASNet is applied. 
Instead of training the entire network, small networks are randomly selected, and sparse coding is used to create sparse connections for model interpretability. 
Training continues until convergence, with validation data used to monitor errors and prevent overfitting through early stopping.
PAGE-Net statistically outperformed Cox-EN with histopathological images only and Cox-PASNet with genomic data only. 

% Pathomic Fusion
In the model proposed in \cite{chen2020pathomic}, the cell graphs from histology images are supposed to get the cell-to-cell interactions and cell neighborhood structure.
To build cell graphs, the first step is generating accurate nuclei segmentation, in which a conditional generative adversarial network (cGAN) is used to learn the appropriate loss function for semantic segmentation.
The edge set and adjacency matrix of the graph are constructed using the K-Nearest Neighbors (KNN) algorithm from segmented cell nuclei.
In addition to manually computed statistics, an unsupervised technique called Contrastive Predictive Coding (CPC) is employed to extract 1024-dimensional features from tissue regions centered around each cell.
Graph Convolutional Networks (GCNs) learn abstract feature representations for each node by aggregating feature vectors from their neighborhood through message passing.
In scenarios with a high-dimensional feature space and limited training samples, traditional feedforward neural networks are susceptible to overfitting. 
To address the challenge and apply more robust regularization techniques when training feed-forward networks on high-dimensional, low-sample-size genomics data, the model adopts normalization layers inspired by Self-Normalizing Networks introduced by Klambaeur et al\cite{klambauer2017self}.
Moreover, the Kronecker Product is used to construct a multi-modal representation. 
The feature vectors of histology images, cell graphs, and genomic features undergo matrix outer product operations to create a multi-modal tensor. 
This tensor captures important interactions among these three modalities in terms of single-modal, bimodal, and tri-modal relationships.
Ultimately, a neural network is trained using fully connected layers with the multi-modal tensor as input. 
The central aim of this method is to fuse heterogeneous modalities with distinct structural dependencies, thereby enhancing research and analysis in cancer pathology. 
To mitigate the impact of noisy uni-modal features during multi-modal training, a gating-based attention mechanism\cite{arevalo2017gated} is introduced to control the expressive power of features within each modality. 
When fusing histology images, cell graphs, and genomic features, the gating mechanism helps reduce the feature space's size before performing the Kronecker Product calculation.

% AMMASurv
To integrate the multi-modality data with different weights, the model proposed in \cite{wang2021ammasurv} uses an asymmetrical Transformer encoder.
The main idea to fuse other modality data unevenly is to add the new nodes and edges into the original graphs.
Different from the normal self-attention in Transformer, the noisy genomic nodes cannot impact the image features because they do not have the outgoing edges, which can only improve themselves by the influence of the imaging features.

% GPDBN
GPDBN in \cite{wang2021gpdbn} uses Kronecker product to build inter-modality and intra-modality interactions between pathology and genomic data for cancer prognosis prediction.

% MultiSurv


% MCAT

% PG-TFNet


% HFBSurv
To overcome the limitation of Kronecker product, HFBSurv extended GPDBN mentioned with the factorized bilinear model.

% MMGL

% Lite-ProSENet

% PORPOISE

% GC-SPLeM

% ponet；pathway-based approaches
Apart from this, another approach to add the genotypic information is using the biological pathway databases to teach the model about the hidden biological functionality.
PONET proposed in \cite{qiu2023deep} uses a sparse biological pathway-informed embedding network for gene expression, additionally adopting the Multi-modal Factorized Bilinear pooling (MFB) method instead of original bilinear model to generate unimodal fusion to catch the modality-specific representations.
Getting each uni-modal fusion, the model can use the output representations as the input of the bimodal and tri-modal fusion respectively utilizing the bimodal attention and tri-modal attention.
Finally the model is trained through the Cox partial likelihood loss proposed by \cite{cheerla2019deep} used for the multi-modalities survival prediction to get the prediction results.

% HGCN

% DeepCoxSC




\subsection{WSI Features Fusion}

Use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). This will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to manually change equation references.

Do not use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed.

{\BibTeX} only functions in conjunction with local .bib files. If you use {\BibTeX} to produce the
bibliography you must attach the .bib files.

{\LaTeX} can't read your mind. If you assign the same label to both a
subsubsection and a table, you may find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| and it might stop a
wanted equation number in the surrounding equation.

If you are submitting your paper to a colorized journal, you can use
the following two lines at the start of the article to ensure its
appearance resembles the final copy:

\smallskip\noindent
\begin{small}
\begin{tabular}{l}
\verb+\+\texttt{documentclass[journal,twoside,web]\{ieeecolor\}}\\
\verb+\+\texttt{usepackage\{\textit{Journal\_Name}\}}
\end{tabular}
\end{small}

\section{Units}
Use either SI (MKS) or CGS as primary units. (SI units are strongly 
encouraged.) English units may be used as secondary units (in parentheses). 
For example, write ``1 kg (2.2lb).'' An exception exists for when 
English units are used as identifiers in commercial products, such as a ``3\textonehalf-in 
disk drive.'' Avoid combining SI and CGS units, such as current in amperes 
and magnetic field in oersteds. This often leads to confusion because 
equations do not balance dimensionally. If you must use mixed units, clearly 
state the units for each quantity in an equation.

The SI unit for magnetic field strength $H$ is A/m. However, if you wish to use 
units of T, either refer to magnetic flux density $B$ or magnetic field 
strength symbolized as $\mu _{0}H$. Use the center dot to separate 
compound units, e.g., ``A$\cdot $m$^{2}$.''


\section{Discussion and future trends}
\label{sec:guidelines}

\subsection{Challenges in histopathology image analysis}
The following list outlines the different types of graphics published in 
IEEE journals. They are categorized based on their construction, and use of 
color~/~shades of gray:

\subsubsection{Model interpretability}
{Figures that are meant to appear in color, or shades of black/gray. Such 
figures may include photographs, illustrations, multicolor graphs, and 
flowcharts.}

\subsubsection{Clinical translation}
{Figures that are composed of only black lines and shapes. These figures 
should have no shades or half-tones of gray, only black and white.}

\subsubsection{Author photos}
{Not allowed for papers in TMI.}

\subsubsection{Tables}
{Data charts which are typically black and white, but sometimes include 
color.}

\begin{table}
\caption{Units for Magnetic Properties}
\label{table}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|p{25pt}|p{75pt}|p{115pt}|}
\hline
Symbol& 
Quantity& 
Conversion from Gaussian and \par CGS EMU to SI $^{\mathrm{a}}$ \\
\hline
$\Phi $& 
magnetic flux& 
1 Mx $\to  10^{-8}$ Wb $= 10^{-8}$ V$\cdot $s \\
$B$& 
magnetic flux density, \par magnetic induction& 
1 G $\to  10^{-4}$ T $= 10^{-4}$ Wb/m$^{2}$ \\
$H$& 
magnetic field strength& 
1 Oe $\to  10^{3}/(4\pi )$ A/m \\
$m$& 
magnetic moment& 
1 erg/G $=$ 1 emu \par $\to 10^{-3}$ A$\cdot $m$^{2} = 10^{-3}$ J/T \\
$M$& 
magnetization& 
1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 10^{3}$ A/m \\
4$\pi M$& 
magnetization& 
1 G $\to  10^{3}/(4\pi )$ A/m \\
$\sigma $& 
specific magnetization& 
1 erg/(G$\cdot $g) $=$ 1 emu/g $\to $ 1 A$\cdot $m$^{2}$/kg \\
$j$& 
magnetic dipole \par moment& 
1 erg/G $=$ 1 emu \par $\to 4\pi \times  10^{-10}$ Wb$\cdot $m \\
$J$& 
magnetic polarization& 
1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 4\pi \times  10^{-4}$ T \\
$\chi , \kappa $& 
susceptibility& 
1 $\to  4\pi $ \\
$\chi_{\rho }$& 
mass susceptibility& 
1 cm$^{3}$/g $\to  4\pi \times  10^{-3}$ m$^{3}$/kg \\
$\mu $& 
permeability& 
1 $\to  4\pi \times  10^{-7}$ H/m \par $= 4\pi \times  10^{-7}$ Wb/(A$\cdot $m) \\
$\mu_{r}$& 
relative permeability& 
$\mu \to \mu_{r}$ \\
$w, W$& 
energy density& 
1 erg/cm$^{3} \to  10^{-1}$ J/m$^{3}$ \\
$N, D$& 
demagnetizing factor& 
1 $\to  1/(4\pi )$ \\
\hline
\multicolumn{3}{p{251pt}}{Vertical lines are optional in tables. Statements that serve as captions for 
the entire table do not need footnote letters. }\\
\multicolumn{3}{p{251pt}}{$^{\mathrm{a}}$Gaussian units are the same as cg emu for magnetostatics; Mx 
$=$ maxwell, G $=$ gauss, Oe $=$ oersted; Wb $=$ weber, V $=$ volt, s $=$ 
second, T $=$ tesla, m $=$ meter, A $=$ ampere, J $=$ joule, kg $=$ 
kilogram, H $=$ henry.}
\end{tabular}
\label{tab1}
\end{table}

\subsection{Multipart figures}
Multipart figures are comprised of more than one sub-figure presented together.
If a multipart figure is made up of multiple figure types (one part is lineart,
and another is grayscale or color) the figure should meet the strictest applicable guidelines.

\subsection{File Formats For Graphics}
\label{formats}
Format and save your graphics as one of the following approved file types:
PostScript (.PS), Encapsulated PostScript (.EPS), Tagged Image File Format (.TIFF),
Portable Document Format (.PDF), Portable Network Graphics (.PNG), or Metapost (.MPS).
After the paper is accepted, any included graphics must be submitted alongside the final manuscript files.

\subsection{Sizing of Graphics}
Most charts, graphs, and tables are one column wide (3.5 inches~/~88 
millimeters) or page wide (7.16 inches~/~181 millimeters). The maximum
depth of a graphic is 8.5 inches (216 millimeters). When choosing the depth of a graphic,
please allow space for a caption. Authors are allowed to size figures between column and
page widths, but it is recommended not to size figures less than column width unless necessary. 

\subsection{Resolution}
The proper resolution of your figures will depend on the type of figure it 
is as defined in the ``Types of Figures'' section. Author photographs, 
color, and grayscale figures should be at least 300dpi. Lineart, including 
tables should be a minimum of 600dpi.

\subsection{Vector Art}
While IEEE does accept and even recommends that authors submit artwork
in vector format, it is our policy is to rasterize all figures for publication. This is done
in order to preserve figures' integrity across multiple computer platforms.

\subsection{Colorspace}
The term colorspace refers to the entire sum of colors that can be 
represented within a given medium. For our purposes, the three main colorspaces
are grayscale, RGB (red/green/blue) and CMYK (cyan/magenta/yellow/black).
RGB is generally used with on-screen graphics, whereas CMYK is used for printing purposes.

All color figures should be generated in RGB or CMYK colorspace. Grayscale 
images should be submitted in grayscale colorspace. Line art may be 
provided in grayscale OR bitmap colorspace. Note that ``bitmap colorspace'' 
and ``bitmap file format'' are not the same thing. When bitmap colorspace 
is selected, .TIF/.TIFF are the recommended file formats.

\subsection{Accepted Fonts Within Figures}
When preparing your graphics IEEE suggests that you use of one of the 
following Open Type fonts: Times New Roman, Helvetica, Arial, Cambria, and 
Symbol. If you are supplying EPS, PS, or PDF files all fonts must be 
embedded. Some fonts may only be native to your operating system; without 
the fonts embedded, parts of the graphic may be distorted or missing.

A safe option when finalizing your figures is to strip out the fonts before 
you save the files, creating ``outline'' type. This converts fonts to 
artwork that will appear uniformly on any screen.

\subsection{Using Labels Within Figures}

\subsubsection{Figure Axis labels}
Figure axis labels are often a source of confusion. Use words rather than 
symbols. As an example, write the quantity ``Magnetization,'' or 
``Magnetization M,'' not just ``M.'' Put units in parentheses. Do not label 
axes only with units. As in Fig. 1, for example, write ``Magnetization 
(A/m)'' or ``Magnetization (A$\cdot$m$^{-1}$),'' not just ``A/m.''
Do not label axes with a ratio of quantities and units.
For example, write ``Temperature (K),'' not ``Temperature/K.'' 

Multipliers can be especially confusing. Write ``Magnetization (kA/m)'' or 
``Magnetization (10$^{3}$ A/m).'' Do not write ``Magnetization 
(A/m)$\,\times\,$1000'' because the reader would not know whether the top 
axis label in Fig. 1 meant 16000 A/m or 0.016 A/m. Figure labels should be 
legible, approximately 8 to 10 point type.

\subsubsection{Subfigure Labels in Multipart Figures and Tables}
Multipart figures should be combined and labeled before final submission. 
Labels should appear centered below each subfigure in 8 point Times New 
Roman font in the format of (a) (b) (c).

\subsection{Referencing a Figure or Table Within Your Paper}
When referencing your figures and tables within your paper, use the 
abbreviation ``Fig.'' even at the beginning of a sentence. Do not abbreviate 
``Table.'' Tables should be numbered with Roman numerals.

\subsection{Submitting Your Graphics}
Format your paper with the graphics included within the body of the text
as you would expect to see the paper in print. Please do this at each stage of the review,
from first submission to final files. For final files only, after the paper has been accepted
for publication, figures should also be submitted individually in addition to the manuscript
file using one of the approved file formats. Place a figure caption below each figure;
place table titles above the tables. Do not include captions or borders in the uploaded figure files.

\subsection{File Naming}
Figures (line artwork or images) should be named starting with the 
first 5 letters of the corresponding author's last name. The next characters in the 
filename should be the number that represents the figure's sequential 
location in the article. For example, in author ``Anderson's'' paper,
the first three figures might be named ander1.tif, ander2.tif, and ander3.ps.

Tables should contain only the body of the table (not the caption) and 
should be named similarly to figures, except that `.t' is inserted 
in-between the author's name and the table number. For example, author 
Anderson's first three tables would be named ander.t1.tif, ander.t2.ps, ander.t3.eps.

Author photographs or biographies are not permitted in IEEE TMI papers.

\subsection{Checking Your Figures: The IEEE Graphics Analyzer}
The IEEE Graphics Analyzer enables authors to pre-screen their graphics for 
compliance with IEEE Transactions and Journals standards before submission. 
The online tool, located at \underline{http://graphicsqc.ieee.org/},
allows authors to upload their graphics in order to check that each file is the correct file format,
resolution, size and colorspace; that no fonts are missing or corrupt;
that figures are not compiled in layers or have transparency,
and that they are named according to the IEEE Transactions and Journals naming convention.
At the end of this automated process, authors are provided with 
a detailed report on each graphic within the web applet, as well as by email.

For more information on using the Graphics Analyzer or any other graphics 
related topic, contact the IEEE Graphics Help Desk by e-mail at 
graphics@ieee.org.

\subsection{Color Processing/Printing in IEEE Journals}
All IEEE Transactions, Journals, and Letters allow an author to publish 
color figures on IEEE Xplore\textregistered\ at no charge, and automatically 
convert them to grayscale for print versions. In most journals, figures and 
tables may alternatively be printed in color if an author chooses to do so. 
Please note that this service comes at an extra expense to the author. If 
you intend to have print color graphics, include a note with your final 
paper indicating which figures or tables you would like to be handled that way,
and stating that you are willing to pay the additional fee.

\section{Experiments}
The prognostic accuracy of models was assessed using
Monte Carlo cross-validation.We randomly split our cohort into paired training (80\%) and testing (20\%) sets.

The word ``data'' is plural, not singular. The subscript for the 
permeability of vacuum $\mu _{0}$ is zero, not a lowercase letter 
``o.'' Use the word ``micrometer'' instead of ``micron.'' A graph within a graph is an 
``inset,'' not an ``insert.'' The word ``alternatively'' is preferred to the 
word ``alternately'' (unless you really mean something that alternates). Use 
the word ``whereas'' instead of ``while'' (unless you are referring to 
simultaneous events). Do not use the word ``essentially'' to mean 
``approximately'' or ``effectively.'' Do not use the word ``issue'' as a 
euphemism for ``problem.'' When compositions are not specified, separate 
chemical symbols by en-dashes; for example, ``NiMn'' indicates the 
intermetallic compound Ni$_{0.5}$Mn$_{0.5}$ whereas 
``Ni--Mn'' indicates an alloy of some composition 
Ni$_{x}$Mn$_{1-x}$.

Be aware of the different meanings of the homophones ``affect'' (usually a 
verb) and ``effect'' (usually a noun), ``complement'' and ``compliment,'' 
``discreet'' and ``discrete,'' ``principal'' (e.g., ``principal 
investigator'') and ``principle'' (e.g., ``principle of measurement''). Do 
not confuse ``imply'' and ``infer.'' 

Prefixes such as ``non,'' ``sub,'' ``micro,'' ``multi,'' and ``ultra'' are 
not independent words; they should be joined to the words they modify, 
usually without a hyphen. There is no period after the ``et'' in the Latin 
abbreviation ``\emph{et al.}'' (it is also italicized). The abbreviation ``i.e.,'' means 
``that is,'' and the abbreviation ``e.g.,'' means ``for example'' (these 
abbreviations are not italicized).

A general IEEE styleguide is available at \underline{http://www.ieee.org/web/publications/authors/transjnl/index.ht}
\discretionary{}{}{}\underline{ml}.

\section{Conclusion}
A conclusion section is not required. Although a conclusion may review the 
main points of the paper, do not replicate the abstract as the conclusion.
A conclusion might elaborate on the importance of the work or suggest 
applications and extensions.

\appendices

\section*{Appendix and the Use of Supplemental Files}
Appendices, if needed, appear before the acknowledgment. If an appendix is not
critical to the main message of the manuscript and is included only for thoroughness
or for reader reference, then consider submitting appendices as supplemental materials.
Supplementary files are available to readers through IEEE \emph{Xplore\textregistered}
at no additional cost to the authors but they do not appear in print versions.
Supplementary files must be uploaded in ScholarOne as supporting documents, but for
accepted papers they should be uploaded as Multimedia documents. Refer readers
to the supplementary files where appropriate within the manuscript text using footnotes.
\footnote{Supplementary materials are available in the supporting documents/multimedia tab.
Further instructions on footnote usage are in the Footnotes section on the next page.}

\section*{Acknowledgment}
The preferred spelling of the word ``acknowledgment'' in American English is 
without an ``e'' after the ``g.'' Use the singular heading even if you have 
many acknowledgments. Avoid expressions such as ``One of us (S.B.A.) would 
like to thank $\ldots$ .'' Instead, write ``F. A. Author thanks $\ldots$ .'' In most 
cases, sponsor and financial support acknowledgments are placed in the 
unnumbered footnote on the first page, not here.


\bibliography{ref}
\bibliographystyle{unsrt}
\end{document}

